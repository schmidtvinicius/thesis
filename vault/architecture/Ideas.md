- A lot of streaming frameworks speak SQL, so maybe that would be the best way of unifying experiments across different frameworks
- [EXPOSE: Experimental Performance Evaluation of Stream Processing Engines Made Easy](zotero://select/library/collections/NNCK7D2G/items/XK4S92KQ) has a really interesting approach to unifying their experiments by creating wrappers around each framework. Could be useful for our own design
- Focus should be on Lakehouses, so we need to make sure our benchmark is centered around them and that it is flexible in terms of adding new Lakehouses. Probably the best way to achieve this is to combine the modularity presented in [LST-Bench]([Link to paper](zotero://select/library/collections/S6HVLU6N/items/7B39LXPC)), but instead of having engines such as spark and trino, we replace that by the stream processing engines (SPE) that will be tested. This makes the most sense, because we can focus on the lakehouses (which are the main focus of the project) and it's up to the user to provide a SPE that can integrate (potentially through connectors) with the lakehouses to run the tasks defined in our benchmark.
- In terms of tasks, I think using the [NEXMark benchmark](https://github.com/nexmark/nexmark) will be quite interesting, especially if we follow the specifications provided [here](https://beam.apache.org/documentation/sdks/java/testing/nexmark/). In addition, we could look at Senska for some other queries, as well as taking inspiration from industry use-cases at Sogeti.
- **It's important to stress that this benchmark is not only about measuring raw throughput and latency of SPE that sink to lakehouses, but rather looking at those metrics under realistic use cases of SPEs.**
- Running raw SQL on streaming engines:
	- https://spark.apache.org/docs/latest/streaming/apis-on-dataframes-and-datasets.html