---
annotation-target: paper.pdf
---


>%%
>```annotation-json
>{"created":"2025-06-18T12:57:46.009Z","updated":"2025-06-18T12:57:46.009Z","document":{"title":"paper.pdf","link":[{"href":"urn:x-pdf:7fd6e4c3a124a9cecb9e1b93650b3f99"},{"href":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf"}],"documentFingerprint":"7fd6e4c3a124a9cecb9e1b93650b3f99"},"uri":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","target":[{"source":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","selector":[{"type":"TextPositionSelector","start":1080,"end":1741},{"type":"TextQuoteSelector","exact":" Although the necessity for a logical and a physical organisation of data lakes in order to meet those requirements is widely recognized, no concrete guidelines are yet provided. The most common architecture implementing this conceptual organisation is the zone architecture, where data is assigned to a certain zone depending on the degree of processing. This paper discusses how FAIR Digital Objects can be used in a novel approach to organize a data lake based on data types instead of zones, how they can be used to abstract the physical implementation, and how they empower generic and portable processing capabilities based on a provenance-based approach.","prefix":"rmance) processing capabilities.","suffix":"1.  INTRODUCTIONData pipelines a"}]}]}
>```
>%%
>*%%PREFIX%%rmance) processing capabilities.%%HIGHLIGHT%% ==Although the necessity for a logical and a physical organisation of data lakes in order to meet those requirements is widely recognized, no concrete guidelines are yet provided. The most common architecture implementing this conceptual organisation is the zone architecture, where data is assigned to a certain zone depending on the degree of processing. This paper discusses how FAIR Digital Objects can be used in a novel approach to organize a data lake based on data types instead of zones, how they can be used to abstract the physical implementation, and how they empower generic and portable processing capabilities based on a provenance-based approach.== %%POSTFIX%%1.  INTRODUCTIONData pipelines a*
>%%LINK%%[[#^lth4uhg8me|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lth4uhg8me


>%%
>```annotation-json
>{"created":"2025-06-18T12:57:58.992Z","updated":"2025-06-18T12:57:58.992Z","document":{"title":"paper.pdf","link":[{"href":"urn:x-pdf:7fd6e4c3a124a9cecb9e1b93650b3f99"},{"href":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf"}],"documentFingerprint":"7fd6e4c3a124a9cecb9e1b93650b3f99"},"uri":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","target":[{"source":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","selector":[{"type":"TextPositionSelector","start":5547,"end":6213},{"type":"TextQuoteSelector","exact":"In  this  paper  we  discuss  (i)  how  the  strong  association  between  a  single  data  entity  and  its  associated  metadata  can  be  expressed  within  a  FAIR  Digital  Object  [13]  in  particular  within  the  context  of  the  data  lake, (ii) how the usage of typed digital objects can conceptually organize a data lake architecture, (iii) how packaging  completely  generic  analysis  tasks  within  Fair  Digital  Objects  can  help  to  monitor  fain  grained  provenance information, and (iv) how exploitation of these concepts in the context of Canonical Workflow Frameworks for Research (CWFR)c can help to increase cross discipline collaboration.","prefix":"otentially rendering it useless.","suffix":"c  https://osf.io/9e3vc/Download"}]}]}
>```
>%%
>*%%PREFIX%%otentially rendering it useless.%%HIGHLIGHT%% ==In  this  paper  we  discuss  (i)  how  the  strong  association  between  a  single  data  entity  and  its  associated  metadata  can  be  expressed  within  a  FAIR  Digital  Object  [13]  in  particular  within  the  context  of  the  data  lake, (ii) how the usage of typed digital objects can conceptually organize a data lake architecture, (iii) how packaging  completely  generic  analysis  tasks  within  Fair  Digital  Objects  can  help  to  monitor  fain  grained  provenance information, and (iv) how exploitation of these concepts in the context of Canonical Workflow Frameworks for Research (CWFR)c can help to increase cross discipline collaboration.== %%POSTFIX%%c  https://osf.io/9e3vc/Download*
>%%LINK%%[[#^yi6w2e2apgl|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^yi6w2e2apgl


>%%
>```annotation-json
>{"created":"2025-06-18T12:58:25.712Z","updated":"2025-06-18T12:58:25.712Z","document":{"title":"paper.pdf","link":[{"href":"urn:x-pdf:7fd6e4c3a124a9cecb9e1b93650b3f99"},{"href":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf"}],"documentFingerprint":"7fd6e4c3a124a9cecb9e1b93650b3f99"},"uri":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","target":[{"source":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","selector":[{"type":"TextPositionSelector","start":9383,"end":9860},{"type":"TextQuoteSelector","exact":" We  put  a  special  focus  on  the  traceability  and  reproducibility  of  the  different  processing  steps,  like  data  ingestion  or  analysis  operations.  Since  the  presented  data  lake  implementation  is  a  generic  framework  aiming  at  supporting  various  use  cases,  we  separate  the  implementation  of  the  aforementioned  processing  steps  from  the  data  lake  implementation  and  thus  make it configurable and extendable for individual use cases","prefix":"te  intensive  research  areas. ","suffix":". In addition, the data lake inc"}]}]}
>```
>%%
>*%%PREFIX%%te  intensive  research  areas.%%HIGHLIGHT%% ==We  put  a  special  focus  on  the  traceability  and  reproducibility  of  the  different  processing  steps,  like  data  ingestion  or  analysis  operations.  Since  the  presented  data  lake  implementation  is  a  generic  framework  aiming  at  supporting  various  use  cases,  we  separate  the  implementation  of  the  aforementioned  processing  steps  from  the  data  lake  implementation  and  thus  make it configurable and extendable for individual use cases== %%POSTFIX%%. In addition, the data lake inc*
>%%LINK%%[[#^w119xo8tq7j|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^w119xo8tq7j


>%%
>```annotation-json
>{"created":"2025-06-18T13:00:20.515Z","updated":"2025-06-18T13:00:20.515Z","document":{"title":"paper.pdf","link":[{"href":"urn:x-pdf:7fd6e4c3a124a9cecb9e1b93650b3f99"},{"href":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf"}],"documentFingerprint":"7fd6e4c3a124a9cecb9e1b93650b3f99"},"uri":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","target":[{"source":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","selector":[{"type":"TextPositionSelector","start":10261,"end":10642},{"type":"TextQuoteSelector","exact":"The core our data lake implementation is the central web application, which i.a. provides the REST-API of the data lake. It also implements the orchestration functions, so that a consistent state of the data lake and the essential minimum of (meta)data quality is being enforced. Here, the actual mapping between the logical and the physical organization of the data lakes happens.","prefix":"akes3.1  Arc  hitecture Overview","suffix":"A schematic view is provided in "}]}]}
>```
>%%
>*%%PREFIX%%akes3.1  Arc  hitecture Overview%%HIGHLIGHT%% ==The core our data lake implementation is the central web application, which i.a. provides the REST-API of the data lake. It also implements the orchestration functions, so that a consistent state of the data lake and the essential minimum of (meta)data quality is being enforced. Here, the actual mapping between the logical and the physical organization of the data lakes happens.== %%POSTFIX%%A schematic view is provided in*
>%%LINK%%[[#^hfwxiv8c88f|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^hfwxiv8c88f


>%%
>```annotation-json
>{"created":"2025-06-18T13:01:44.770Z","updated":"2025-06-18T13:01:44.770Z","document":{"title":"paper.pdf","link":[{"href":"urn:x-pdf:7fd6e4c3a124a9cecb9e1b93650b3f99"},{"href":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf"}],"documentFingerprint":"7fd6e4c3a124a9cecb9e1b93650b3f99"},"uri":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","target":[{"source":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","selector":[{"type":"TextPositionSelector","start":11194,"end":11409},{"type":"TextQuoteSelector","exact":"There is, however, one exception: The direct interaction of users with version control tools, like GitLab, is tightly integrated into our concept of a data lake in order to support widely used development workflows.","prefix":"isting workflows and pipelines. ","suffix":"Figure 1.  Schematic view of the"}]}]}
>```
>%%
>*%%PREFIX%%isting workflows and pipelines.%%HIGHLIGHT%% ==There is, however, one exception: The direct interaction of users with version control tools, like GitLab, is tightly integrated into our concept of a data lake in order to support widely used development workflows.== %%POSTFIX%%Figure 1.  Schematic view of the*
>%%LINK%%[[#^dua3jrfi2qp|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^dua3jrfi2qp


>%%
>```annotation-json
>{"created":"2025-06-18T13:05:24.002Z","updated":"2025-06-18T13:05:24.002Z","document":{"title":"paper.pdf","link":[{"href":"urn:x-pdf:7fd6e4c3a124a9cecb9e1b93650b3f99"},{"href":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf"}],"documentFingerprint":"7fd6e4c3a124a9cecb9e1b93650b3f99"},"uri":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","target":[{"source":"vault:/Realising Data-Centric Scientific Workflows with Provenance-Capturing on Data Lakes/paper.pdf","selector":[{"type":"TextPositionSelector","start":16674,"end":17251},{"type":"TextQuoteSelector","exact":"In order to achieve a consistent metadata quality despite a constantly evolving metadata schema, a rather simple modeling approach is being taken. First, four top level data types are defined, raw data, processed data, container and manifest, from which other data types can be derived. Users can register derived data types by defining a new schema which consists of the associated attributes and an assigned name for the new  data  type. Through  this,  only  known  data  types  with  known  and  typed  attributes  are  being  indexed  and  ingested  into  the  data  lake.","prefix":"  Cons  tantly Evolving Metadata","suffix":"  Based  on  this  environment  "}]}]}
>```
>%%
>*%%PREFIX%%Cons  tantly Evolving Metadata%%HIGHLIGHT%% ==In order to achieve a consistent metadata quality despite a constantly evolving metadata schema, a rather simple modeling approach is being taken. First, four top level data types are defined, raw data, processed data, container and manifest, from which other data types can be derived. Users can register derived data types by defining a new schema which consists of the associated attributes and an assigned name for the new  data  type. Through  this,  only  known  data  types  with  known  and  typed  attributes  are  being  indexed  and  ingested  into  the  data  lake.== %%POSTFIX%%Based  on  this  environment*
>%%LINK%%[[#^bv31c7rt4ib|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^bv31c7rt4ib
